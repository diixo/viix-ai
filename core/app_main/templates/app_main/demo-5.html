
{% extends "app_main/base.html" %}

{% block meta %}
<title>Demo fonts</title>
<meta property="og:title" content="Scheduler">
<meta name="description" content="Scheduler">
<meta property="og:description" content="Scheduler">
{% endblock %}

{% block content %}

<div class="col-md-10 col-12 demo-assistant bg-white px-3 py-4 rounded-3">

<h5 class="fw-bold text-center" style="font-family: 'Assistant';">Language Models are Few-Shot learners</h5>

<h6 class="fw-bold mb-3 text-center" style="font-family: 'Assistant';">Language Models are Few-Shot learners</h6>


Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. 
While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. 
By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. 
Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. 
Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. 
For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. 
GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks 
that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic.
At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. 
Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.


<h6 class="fw-bold text-center m-2" style="font-family: 'Assistant';">Training Dataset</h6>

Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2
[RSR+19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same
sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have
lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets:
(1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference
corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy
and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added
known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.
Details of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added
several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected
by scraping links over a longer period of time, and first described in [KMH+20], two internet-based books corpora
(Books1 and Books2) and English-language Wikipedia. A major methodological concern with language models pretrained on a broad swath of internet data, particularly large
models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by
having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched
for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.
Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible
to retrain the model.
</div>

{% endblock %}
